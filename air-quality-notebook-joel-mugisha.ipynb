{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e852394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.metrics import MeanAbsoluteError, RootMeanSquaredError\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f92e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading file dropping NO column since it is not useful\n",
    "# converign datetime column to datetime and setting as index\n",
    "data = pd.read_csv('pm.csv')\n",
    "data = data.iloc[24:]\n",
    "data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "data = data.set_index('datetime')\n",
    "data.drop(columns=['No'], inplace=True)\n",
    "data['pm2.5'] = data['pm2.5'].interpolate(method='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f02e754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting the data\n",
    "# taking last 6100 rows for validation set and rest for train\n",
    "train_set = data[:-6100]\n",
    "validation_set = data[-6100:]\n",
    "\n",
    "# defingin scalers\n",
    "features_scaler = MinMaxScaler()\n",
    "target_scaler = MinMaxScaler()\n",
    "\n",
    "# train set features and target split\n",
    "train_features = train_set.drop(columns=['pm2.5'])\n",
    "train_target = train_set['pm2.5'].values.reshape(-1, 1)\n",
    "val_features = validation_set.drop(columns=['pm2.5'])\n",
    "val_target = validation_set['pm2.5'].values.reshape(-1, 1)\n",
    "\n",
    "# scaling features and target between 0 and 1\n",
    "train_features_scaled = features_scaler.fit_transform(train_features.values)\n",
    "train_target_scaled = target_scaler.fit_transform(train_target)\n",
    "val_features_scaled = features_scaler.transform(val_features.values)\n",
    "val_target_scaled = target_scaler.transform(val_target)\n",
    "\n",
    "# creating timeseries generators for train and validation sets\n",
    "train_series = TimeseriesGenerator(train_features_scaled,  train_target_scaled ,sampling_rate=1 ,length=96, batch_size=32)\n",
    "val_series = TimeseriesGenerator(val_features_scaled, val_target_scaled, sampling_rate=1, length=96, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df721bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = Sequential([\n",
    "    LSTM(128, activation='tanh', return_sequences=True, input_shape=(96, train_features_scaled.shape[1])),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64, activation='tanh', return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.02)),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# model fit and compile \n",
    "# note the RMSR is for scaled values betwween 0-1\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=[MeanAbsoluteError()])\n",
    "model.summary()\n",
    "history = model.fit( train_series, validation_data=val_series, epochs=35, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de94ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading file, removing NO column as before\n",
    "# setting index as datatime and tranforming to datatime type\n",
    "# taking the last 96 rows from train dataset since the model will use them to create the first prediction\n",
    "# also removing pm2.5 from last_96\n",
    "# the making a final df from last 96 + test csv\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_dates = pd.to_datetime(df_test['datetime'])\n",
    "df_test['datetime'] = pd.to_datetime(df_test['datetime'])\n",
    "df_test = df_test.set_index('datetime')\n",
    "df_test.drop(columns=['No',], inplace=True)\n",
    "last_rows = data[-96:].copy()\n",
    "last_rows.drop(columns=['pm2.5',], inplace=True)\n",
    "df_test = pd.concat([last_rows, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97725136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tranforming the final test df\n",
    "# and creating a time series \n",
    "df_test_scaled = features_scaler.transform(df_test.values)\n",
    "test_series = TimeseriesGenerator(df_test_scaled, df_test_scaled, 96, sampling_rate=1, batch_size=320)\n",
    "predictions = model.predict(test_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab71fe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inversing the prediction from 0-1 to orignal scale\n",
    "# and creating submission file \n",
    "actual_predictions = target_scaler.inverse_transform(predictions)\n",
    "actual_predictions_int = np.round(actual_predictions).astype(int)\n",
    "df_dates = df_dates.dt.strftime('%Y-%m-%d %#H:%M:%S')\n",
    "\n",
    "final_cvs = pd.DataFrame({\n",
    "    'row ID': df_dates,\n",
    "    'pm2.5': actual_predictions_int.flatten()\n",
    "})\n",
    "\n",
    "final_cvs.to_csv('joel-predictions.csv', index=False)\n",
    "print(final_cvs.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
